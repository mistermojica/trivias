import os
from moviepy.editor import concatenate_videoclips, VideoFileClip, ImageClip, TextClip, CompositeVideoClip, AudioFileClip, CompositeAudioClip, VideoClip
from moviepy.video.fx.all import fadein, fadeout
import boto3
import time
import uuid
import io
import json
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from openai import OpenAI
import subprocess
import argparse
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())

uuidcode = ""  # Genera un UUID 칰nico para archivos temporales

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")

openai = OpenAI()

openai.api_key = OPENAI_API_KEY

voices = [
    {"Engine": "generative", "LanguageCode": "en-US", "VoiceId": "Matthew", "Gender": "Male", "TextType": "text", "Newscaster": ""},
    {"Engine": "generative", "LanguageCode": "en-US", "VoiceId": "Ruth", "Gender": "Female", "TextType": "text", "Newscaster": ""},
    {"Engine": "long-form", "LanguageCode": "en-US", "VoiceId": "Danielle", "Gender": "Female", "TextType": "text", "Newscaster": ""},
    {"Engine": "long-form", "LanguageCode": "en-US", "VoiceId": "Gregory", "Gender": "Male", "TextType": "text", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Danielle", "Gender": "Female", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Gregory", "Gender": "Male", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Ivy", "Gender": "Female", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Joanna", "Gender": "Female", "TextType": "ssml", "Newscaster": "news"},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Kendra", "Gender": "Female", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Kimberly", "Gender": "Female", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Salli", "Gender": "Female", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Joey", "Gender": "Male", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Justin", "Gender": "Male", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Kevin", "Gender": "Male", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Matthew", "Gender": "Male", "TextType": "ssml", "Newscaster": "news"},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Ruth", "Gender": "Female", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "en-US", "VoiceId": "Stephen", "Gender": "Male", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "neural", "LanguageCode": "es-US", "VoiceId": "Lupe", "Gender": "Female", "TextType": "ssml", "Newscaster": "news"},
    {"Engine": "neural", "LanguageCode": "es-US", "VoiceId": "Pedro", "Gender": "Male", "TextType": "ssml", "Newscaster": ""},
    {"Engine": "standard", "LanguageCode": "es-US", "VoiceId": "Miguel", "Gender": "Male", "TextType": "ssml", "Newscaster": ""}
]

def get_polly_response(engine, voiceid, text, prosodyrate="100%"):
    # print("get_polly_response:", engine, voiceid, text, prosodyrate)

    # Coloca tu lista de voces aqu칤
    voice = next((v for v in voices if v["Engine"] == engine and v["VoiceId"] == voiceid), None)

    # print(voice)

    if not voice:
        raise ValueError(f"Voice with Engine '{engine}' and VoiceId '{voiceid}' not found.")

    text_type = voice["TextType"]
    language_code = voice["LanguageCode"]
    newscaster = voice["Newscaster"]

    if text_type == "text":
        polly_text = text
    elif text_type == "ssml":
        if newscaster == "news":
            polly_text = f'<speak><prosody rate="{prosodyrate}"><amazon:domain name="news">{text}</amazon:domain></prosody></speak>'
        else:
            polly_text = f'<speak><prosody rate="{prosodyrate}">{text}</prosody></speak>'
            # polly_text = f'<speak><prosody rate="{prosodyrate}"><amazon:domain name="conversational">{text}</amazon:domain></prosody></speak>'
    else:
        raise ValueError("Invalid TextType")

    polly_client = boto3.Session(profile_name='doccumi', region_name="us-east-1").client("polly")

    # print("get_polly_response:", polly_text, text_type, voiceid, language_code)

    response = polly_client.synthesize_speech(
        Engine=engine,
        OutputFormat="ogg_vorbis",
        Text=polly_text,
        TextType=text_type,
        VoiceId=voiceid,
        LanguageCode=language_code,
    )

    return response

def create_file_path(file_path):
    # print("file_path:", file_path)
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)
        print(f"Ruta creada: {directory}")
    # else:
    #     print(f"La ruta ya existe: {directory}")

def text_to_speech_polly(text, output_filename, voz, max_retries=7):
    create_file_path(output_filename)

    attempt = 0
    success = False
    while not success and attempt < max_retries:
        try:
            response = get_polly_response("neural", voz, text, "100%")
            audio_data = response["AudioStream"].read()

            with open(output_filename, "wb") as out:
                out.write(audio_data)

            success = True
        except Exception as e:
            attempt += 1
            print(f"Error on attempt {attempt}: {e}")
            time.sleep(3)  # Wait a bit before retrying

    if not success:
        raise Exception(f"Failed to get Polly response after {max_retries} attempts")

    return output_filename

# Generar el audio narrativo con AWS Polly
def generate_narration(text, output_file, voz):
    text_to_speech_polly(text, output_file, voz)

# Crear un clip de video de fondo con opacidad
def create_background_video(background_video_path, duration):
    background_clip = VideoFileClip(background_video_path).subclip(0, duration)
    return background_clip.set_opacity(0.5)


def add_logo(logo_path, video_clip):
    # Abrir el logo como imagen Pillow
    logo_image = Image.open(logo_path)

    # Calcular el nuevo tama침o del logo
    final_width = int(video_clip.w * 0.5)
    aspect_ratio = logo_image.height / logo_image.width
    new_height = int(final_width * aspect_ratio)

    # Redimensionar el logo usando Pillow
    logo_resized = logo_image.resize((final_width, new_height), Image.Resampling.LANCZOS)

    # Convertir la imagen redimensionada a ImageClip
    logo_clip = ImageClip(np.array(logo_resized)).set_duration(video_clip.duration)

    # Posicionar el logo en el video
    logo_clip = logo_clip.set_position(("center", 200))  # Agregar un margen de 100 p칤xeles desde la parte superior

    return logo_clip

# A침adir texto de la pregunta
def add_question_text(question_text, video_clip, question_font_path, margin=80, top_margin=450):
    # Calcular el ancho m치ximo permitido para el texto, considerando los m치rgenes
    max_width = video_clip.w - 2 * margin

	    # Crear el TextClip con ajuste de l칤nea
    question_clip = (TextClip(question_text, fontsize=80, color='white', font=question_font_path, method='caption', size=(max_width, None))
                     .set_duration(video_clip.duration)
                     .set_pos(("center", top_margin)))  # Controlar la altura de presentaci칩n con top_margin
    return question_clip

# A침adir opciones de respuesta
def add_options(options, video_clip, options_font_path, margin=170, top_margin=1000):
    # print(f"[DEBUG] add_options - video_clip.duration: {video_clip.duration}")

    top_margin=950
    option_clips = []
    first_option_pos = top_margin

    # Condicional para ajustar el espacio entre opciones
    if len(options) == 3:
        option_space = 170
    elif len(options) == 4:
        option_space = 150
    else:
        option_space = 170  # Valor por defecto

    y_positions = []
    for i in range(len(options)):
        y_position = first_option_pos + (option_space * i)
        y_positions.append(y_position)

    # Configuraci칩n de estilo
    circle_radius = int(40 * 1.5)  # Aumentar el radio del c칤rculo en un 30%
    circle_color = '#6A5ACD'  # Color del c칤rculo en formato hexadecimal (Lavender)
    option_bg_color = 'white'  # Fondo blanco para las opciones
    option_bg_height = 115  # Altura del fondo de las opciones
    corner_radius = 50  # Radio de las esquinas redondeadas del fondo

    for i, option in enumerate(options):
        # Crear el c칤rculo con Pillow
        circle_image = Image.new("RGBA", (circle_radius * 2, circle_radius * 2), (255, 255, 255, 0))
        draw = ImageDraw.Draw(circle_image)
        draw.ellipse((0, 0, circle_radius * 2, circle_radius * 2), fill=circle_color)

        # Convertir la imagen Pillow a un array numpy
        circle_array = np.array(circle_image)

        # Convertir el array numpy a ImageClip
        circle_clip = ImageClip(circle_array).set_duration(video_clip.duration)
        circle_clip = circle_clip.set_position((margin, y_positions[i]))

        # Crear el texto dentro del c칤rculo
        label_text = chr(65 + i)  # Genera las letras A, B, C
        label_clip = (TextClip(label_text, fontsize=80, color='white', font=options_font_path)
                      .set_duration(video_clip.duration)
                      .set_position((margin + 30, y_positions[i] + 10)))  # Ajustar posici칩n del texto dentro del c칤rculo

        # Crear el fondo redondeado para la opci칩n usando Pillow
        bg_width = video_clip.w - 2 * margin
        bg_image = Image.new("RGBA", (bg_width, option_bg_height), (255, 255, 255, 0))
        rounded_rectangle = Image.new("RGBA", (bg_width, option_bg_height), option_bg_color)
        mask = Image.new("L", (bg_width, option_bg_height), 0)
        mask_draw = ImageDraw.Draw(mask)
        mask_draw.rounded_rectangle([(0, 0), (bg_width, option_bg_height)], corner_radius, fill=255)
        bg_image = Image.composite(rounded_rectangle, bg_image, mask)

        # Convertir el fondo redondeado a ImageClip
        bg_array = np.array(bg_image)
        option_bg_clip = ImageClip(bg_array).set_duration(video_clip.duration)
        option_bg_clip = option_bg_clip.set_position((margin + 10, y_positions[i] + 1))

        # Crear el texto de la opci칩n
        option_text_clip = (TextClip(option, fontsize=70, color='black', font=options_font_path)
                            .set_duration(video_clip.duration)
                            .set_position((margin + circle_radius * 2 + 20, y_positions[i] + 15)))

        option_text_clip.text = option

        # Componer la opci칩n final
        composed_clip = CompositeVideoClip([option_bg_clip, circle_clip, label_clip, option_text_clip], size=(video_clip.w, video_clip.h))
        option_clips.append(composed_clip)

    return option_clips

# Revelar la opci칩n correcta
def reveal_correct_option(options_clips, video_clip, options, correct_option_index, start_time, reveal_time, options_font_path, margin=80):
    # print(f"[DEBUG] Opciones: {options}")
    # print(f"[DEBUG] Clips de Opciones: {options_clips}")
    # print(f"[DEBUG] 칈ndice de Opci칩n Correcta: {correct_option_index}")
    # print(f"[DEBUG] reveal_correct_option - video_clip.duration: {video_clip.duration}")
    # print(f"[DEBUG] reveal_correct_option - start_time: {start_time}")
    # print(f"[DEBUG] reveal_correct_option - reveal_time: {reveal_time}")

    # Obtener el texto de la opci칩n correcta desde el arreglo
    correct_option_text = options[correct_option_index]
    # print(f"[DEBUG] Texto de Opci칩n Correcta: {correct_option_text}")

    # Obtener el clip de la opci칩n correcta
    composite_clip = options_clips[correct_option_index]

    # Encontrar el TextClip que contiene el texto de la opci칩n correcta
    option_text_clip = None
    bg_model_clip = composite_clip.clips[0]
    for clip in composite_clip.clips:
        if isinstance(clip, TextClip) and hasattr(clip, 'text'):
            if clip.text == correct_option_text:
                option_text_clip = clip
                break

    if not option_text_clip:
        raise ValueError("No se encontr칩 un TextClip con el texto de la opci칩n dentro del CompositeVideoClip")

    correct_option_bg_color = 'yellow'
    option_bg_height = 115
    corner_radius = 50
    bg_width = video_clip.w - 2 * margin

    # Crear el fondo amarillo para la opci칩n correcta
    correct_bg_image = Image.new("RGBA", (bg_width, option_bg_height), (255, 255, 255, 0))
    correct_rounded_rectangle = Image.new("RGBA", (bg_width, option_bg_height), correct_option_bg_color)
    correct_mask = Image.new("L", (bg_width, option_bg_height), 0)
    correct_mask_draw = ImageDraw.Draw(correct_mask)
    correct_mask_draw.rounded_rectangle([(0, 0), (bg_width, option_bg_height)], corner_radius, fill=255)
    correct_bg_image = Image.composite(correct_rounded_rectangle, correct_bg_image, correct_mask)

    correct_bg_array = np.array(correct_bg_image)
    # correct_option_bg_clip = ImageClip(correct_bg_array).set_duration(reveal_time).set_position(bg_model_clip.pos).set_start(start_time)

    correct_bg_image_resized = Image.fromarray(correct_bg_array).resize((bg_model_clip.w, bg_model_clip.h), Image.Resampling.LANCZOS)

    # Convertir la imagen redimensionada en un array numpy
    correct_bg_array_resized = np.array(correct_bg_image_resized)

    # Crear el ImageClip con el tama침o ya ajustado
    correct_option_bg_clip = ImageClip(correct_bg_array_resized).set_duration(reveal_time).set_position(bg_model_clip.pos).set_start(start_time)

    # Crear el nuevo CompositeVideoClip que incluye el fondo amarillo y mantiene el fondo blanco hasta el final
    # new_composed_clip = CompositeVideoClip(
    #     composite_clip.clips + [correct_option_bg_clip],
    #     size=(composite_clip.w, composite_clip.h)
    # )

    # Ajustar el clip original para que termine cuando comience correct_option_bg_clip
    new_clips = []
    for clip in composite_clip.clips:
        if isinstance(clip, ImageClip) and clip.size == correct_option_bg_clip.size:
            # Ajustar la duraci칩n del clip original para que termine cuando comience el nuevo clip
            original_clip = clip.set_end(start_time)
            new_clips.append(original_clip)
            # Agregar el clip amarillo que comienza en start_time
            new_clips.append(correct_option_bg_clip)
        else:
            new_clips.append(clip)

    # Crear el nuevo CompositeVideoClip con la lista de clips actualizada
    new_composed_clip = CompositeVideoClip(
        new_clips,
        size=(composite_clip.w, composite_clip.h)
    )

    # Actualizar la lista de clips de opciones con el nuevo clip compuesto para la opci칩n correcta
    options_clips[correct_option_index] = new_composed_clip

    return options_clips

# A침adir el texto de la cuenta
def add_account_text(account_text, video_clip, account_font_path):
    account_clip = (TextClip(account_text, fontsize=50, color='white', font=account_font_path)
                    .set_duration(video_clip.duration)
                    .set_pos(("center", 1700)))
    return account_clip

# A침adir efectos de sonido
def add_sound_effects(tictac_sound_path, start_time, reveal_time):
    tictac_sound = AudioFileClip(tictac_sound_path).subclip(start_time, reveal_time)
    return tictac_sound

# Funci칩n para crear la imagen del emoji y devolverla como un objeto Image de PIL
def create_emoji_image(unicode_text, font_path, constant_font_size, emoji_size):
    # Cargar la fuente con el tama침o especificado
    font = ImageFont.truetype(font_path, constant_font_size)

    # Calcular el tama침o del texto (emoji) utilizando getbbox
    left, top, right, bottom = font.getbbox(unicode_text)
    text_width = right - left
    text_height = bottom - top

    # Crea una nueva imagen con el tama침o del texto
    im = Image.new("RGBA", (text_width, text_height), (0, 0, 0, 0))
    draw = ImageDraw.Draw(im)

    # Dibuja el emoji en la imagen
    draw.text((-left, -top), unicode_text, font=font, embedded_color=True)

    # Redimensionar la imagen al tama침o especificado por el par치metro emoji_size
    im_resized = im.resize((emoji_size, emoji_size), Image.Resampling.LANCZOS)

    # Devuelve la imagen redimensionada como un objeto PIL
    return im_resized

def create_progress_bar_with_emoji(duration, width=800, height=100, scale_height=0.8, emoji_size=100, bar_height=100, bar_color="green", bg_color="black", emoji="游", constant_font_size=137, font_path="", proportion=0.8):
    # Diccionario de colores RGB
    color_dict = {
        "yellow": (255, 255, 0),
        "green": (0, 255, 0),
        "red": (255, 0, 0),
        "blue": (0, 0, 255),
        "white": (255, 255, 255),
        "black": (0, 0, 0),
        "gray": (128, 128, 128),
    }

    # Convertir nombres de colores a valores RGB
    bar_color_rgb = color_dict[bar_color]
    bg_color_rgb = color_dict[bg_color]

    # Crear la imagen del emoji y convertirla en un array numpy
    emoji_image = create_emoji_image(emoji, font_path, constant_font_size, emoji_size)
    emoji_array = np.array(emoji_image)

    # Crear un ImageClip a partir del array numpy
    emoji_clip = ImageClip(emoji_array).set_duration(duration)

    # Reducir la altura total del video seg칰n el par치metro scale_height
    total_height = int((bar_height + emoji_clip.h) * scale_height)  # Nueva altura reducida

    # Definir el tama침o ocupado por la barra de progreso y el emoji (80% del ancho total)
    content_width = int(width * proportion)
    margin = (width - content_width) // 2  # Espacio en blanco a los lados


    def make_frame(t):
        # Crear una imagen de fondo con color s칩lido usando PIL
        img = Image.new("RGBA", (content_width, bar_height), (0, 0, 0, 0))  # Fondo transparente

        draw = ImageDraw.Draw(img)
        radius = bar_height // 2  # El radio es la mitad de la altura para bordes completamente redondeados

        # Dibujar la barra de fondo gris con bordes redondeados
        draw.rounded_rectangle(
            [(0, 0), (content_width, bar_height)],
            radius=radius,
            fill=bg_color_rgb
        )

        # Calcular el ancho de la barra de progreso basada en el tiempo t y la duraci칩n total
        bar_width = int((t / duration) * content_width)

        # Dibujar la barra de progreso con bordes redondeados sobre la barra gris
        draw.rounded_rectangle(
            [(0, 0), (bar_width, bar_height)],
            radius=radius,
            fill=bar_color_rgb
        )

        # Convertir la imagen de PIL a RGB para evitar conflictos de canal alfa
        img = img.convert("RGB")

        # Convertir la imagen de PIL a un array NumPy para MoviePy
        return np.array(img)

    # Crear un VideoClip para la barra de progreso
    progress_bar_clip = VideoClip(make_frame, duration=duration).set_position((margin, (total_height - bar_height) // 2))

    # Calcular la posici칩n vertical del emoji y la barra para que ambos est칠n centrados
    emoji_position_y = (total_height - bar_height) // 2 - emoji_clip.h // 2 + bar_height // 2

    # Alinear el centro del emoji con el borde derecho de la barra de progreso dentro del 80%
    emoji_clip = emoji_clip.set_position(lambda t: (
        margin + int((t / duration) * content_width) - emoji_clip.w // 2,
        emoji_position_y
    ))

    # Componer el clip final con la barra de progreso y el emoji
    final_clip = CompositeVideoClip([
        progress_bar_clip,
        emoji_clip
    ], size=(width, total_height))

    return final_clip


# Componer el video final
def compose_video(video_total_duration, background_clip, logo_clip, question_clip, question_image_clip, options_clips, account_clip, narration_audio, narration_audio_winner, clock_sound_effects, ding_sound_effects, progress_bar_with_emoji):
    final_clip = CompositeVideoClip([background_clip.set_start(0), logo_clip.set_start(0), question_clip.set_start(0), question_image_clip.set_start(0), *options_clips, account_clip.set_start(0), progress_bar_with_emoji.set_start(narration_audio.duration)])
    final_clip.set_duration(video_total_duration)
    print(f"[DEBUG] video_total_duration 1: {video_total_duration}")
    # for idx, clip in enumerate(options_clips):
    #     print(f"[DEBUG] options_clips[{idx}] - start: {clip.start}, duration: {clip.duration}, end: {clip.end}")

    # final_audio = CompositeAudioClip([narration_audio.set_start(0), sound_effects.set_start(narration_audio.duration)])
    final_audio = CompositeAudioClip([
        narration_audio.set_start(0),
        clock_sound_effects.set_start(narration_audio.duration),
        ding_sound_effects.volumex(0.05).set_start(narration_audio.duration + clock_sound_effects.duration),
        narration_audio_winner.set_start(narration_audio.duration + clock_sound_effects.duration + 1)
    ])

    print(f"[DEBUG] narration_audio.duration: {narration_audio.duration}")
    print(f"[DEBUG] clock_sound_effects.duration: {clock_sound_effects.duration}")
    print(f"[DEBUG] ding_sound_effects.duration: {ding_sound_effects.duration}")
    print(f"[DEBUG] narration_audio_winner.duration: {narration_audio_winner.duration}")
    # print(f"[DEBUG] clock_sound_effects.duration: {clock_sound_effects.duration}")
    # print(f"[DEBUG] narration_audio.duration + clock_sound_effects.duration: {narration_audio.duration + clock_sound_effects.duration}")
    # print(f"[DEBUG] final_audio.duration: {final_audio.duration}")

    final_clip = final_clip.set_audio(final_audio)
    print(f"[DEBUG] final_audio.duration 2: {final_audio.duration}")
    print(f"[DEBUG] final_clip.duration 2: {final_clip.duration}")

    # final_clip = final_clip.subclip(0, 4)
    # final_clip.write_videofile(output_file, codec='libx264', fps=24, preset='ultrafast')
    # final_clip.write_videofile(output_file, codec='libx264', fps=24, preset='ultrafast')
    return final_clip

# Funci칩n principal para generar el video de trivia
def generate_trivia_video(main_question, voice, background_video_path, logo_path, question_text, question_image, options, correct_option_index, account_text, narration_text, narration_text_winner, tictac_sound_path, ding_sound_path, question_font_path, options_font_path, account_font_path, question_image_font_path):
    narration_audio_file = f"./audios/{uuidcode}.ogg"
    generate_narration(narration_text, narration_audio_file, voice)
    narration_audio_file_winner = f"./audios/{uuidcode}_winner.ogg"
    generate_narration(narration_text_winner, narration_audio_file_winner, voice)
    narration_audio = AudioFileClip(narration_audio_file)
    narration_audio_winner = AudioFileClip(narration_audio_file_winner)

    clock_sound_effects = add_sound_effects(tictac_sound_path, start_time=0, reveal_time=3)
    video_duration_before_winner = narration_audio.duration + clock_sound_effects.duration
    ding_sound_effects = add_sound_effects(ding_sound_path, start_time=0, reveal_time=2)
    ding_sound_effects.volumex(0.05)
    # video_total_duration = narration_audio.duration + clock_sound_effects.duration + narration_audio_winner.duration
    video_winner_duration = max(narration_audio_winner.duration, ding_sound_effects.duration)
    video_total_duration = narration_audio.duration + clock_sound_effects.duration + max(narration_audio_winner.duration, ding_sound_effects.duration)
    
    print(f"[DEBUG] generate_trivia_video - narration_audio.duration: {narration_audio.duration}")
    print(f"[DEBUG] generate_trivia_video - clock_sound_effects.duration: {clock_sound_effects.duration}")
    print(f"[DEBUG] generate_trivia_video - ding_sound_effects.duration: {ding_sound_effects.duration}")
    print(f"[DEBUG] generate_trivia_video - total: {narration_audio.duration + clock_sound_effects.duration + ding_sound_effects.duration}")
    print(f"[DEBUG] generate_trivia_video - narration_audio_winner.duration: {narration_audio_winner.duration}")
    
    # print(f"[DEBUG] video_duration_before_winner: {video_duration_before_winner}")
    # print(f"[DEBUG] video_winner_duration: {video_winner_duration}")
    print(f"[DEBUG] video_total_duration: {video_total_duration}")

    background_clip = create_background_video(background_video_path, duration=video_total_duration)
    # print(f"[DEBUG] background_clip.duration: {background_clip.duration}")
    logo_clip = add_logo(logo_path, background_clip)
    if (question_image):
        question_clip = add_question_text(main_question, background_clip, question_font_path)
    else:
        question_clip = add_question_text(question_text, background_clip, question_font_path)

    # Funci칩n para guardar la imagen del emoji
    def save_emoji_image(unicode_text, font_path, font_size, output_path):
        # Cargar la fuente con el tama침o especificado
        font = ImageFont.truetype(font_path, font_size)

        # Calcular el tama침o del texto (emoji) utilizando getbbox
        left, top, right, bottom = font.getbbox(unicode_text)
        text_width = right - left
        text_height = bottom - top

        # Aseg칰rate de que las dimensiones sean positivas
        text_width = max(1, text_width)
        text_height = max(1, text_height)

        # Crea una nueva imagen con el tama침o del texto
        im = Image.new("RGBA", (text_width, text_height), (0, 0, 0, 0))
        draw = ImageDraw.Draw(im)

        # Dibuja el emoji en la imagen
        draw.text((-left, -top), unicode_text, font=font, embedded_color=True)

        # Guarda la imagen en un archivo
        im.save(output_path)

    # Condicional para generar el question_image_clip
    if question_image.strip():
        # Generar la imagen del emoji y guardarla
        emoji_image_path = f"./images/{uuidcode}_emoji.png"
        save_emoji_image(question_image, question_image_font_path, 137, emoji_image_path)

        # Crear el ImageClip a partir de la imagen guardada
        question_image_clip = ImageClip(emoji_image_path)
        question_image_clip = question_image_clip.set_duration(question_clip.duration)
        question_image_clip = question_image_clip.set_position(('center', question_clip.size[1] + 450))  # Posici칩n debajo del question_clip
    else:
        # Crear un clip vac칤o si question_image est치 en blanco
        question_image_clip = TextClip(' ', fontsize=150, color='white')
        question_image_clip = question_image_clip.set_duration(question_clip.duration)
        question_image_clip = question_image_clip.set_position(('center', question_clip.size[1] + 500))  # Posici칩n debajo del question_clip

    options_clips = add_options(options, background_clip, options_font_path) #, reveal_time=4
    # Pasa la fuente de opciones a la funci칩n reveal_correct_option
    options_clips = reveal_correct_option(options_clips, background_clip, options, correct_option_index, start_time=video_duration_before_winner, reveal_time=video_winner_duration, options_font_path=options_font_path)
    account_clip = add_account_text(account_text, background_clip, account_font_path)

    # Ejemplo de uso
    progress_bar_with_emoji = create_progress_bar_with_emoji(
        duration=clock_sound_effects.duration,        # Duraci칩n en segundos
        width=int(background_clip.w * 0.8),         # Ancho del video
        height=0,          # Altura inicial del video (no se usa directamente ahora)
        scale_height=0.8,    # Reducir la altura del video al 80%
        emoji_size=70,     # Tama침o del emoji
        bar_height=40,     # Altura de la barra de progreso
        bar_color="yellow",
        bg_color="gray",
        emoji="游",
        constant_font_size=137,     # Tama침o del emoji
        font_path="./assets/fonts/AppleColorEmoji.ttf",
        proportion=0.85    # Espacio que ocupa la barra dentro del recuadro
    )

    progress_bar_with_emoji = progress_bar_with_emoji.set_pos(("center", 1575))

    # exit()

    video_clip = compose_video(video_total_duration, background_clip, logo_clip, question_clip, question_image_clip, options_clips, account_clip, narration_audio, narration_audio_winner, clock_sound_effects, ding_sound_effects, progress_bar_with_emoji)

    # Limpieza de archivos temporales
    # os.remove(narration_audio_file)
    # os.remove(narration_audio_file_winner)

    return video_clip


def beta_generate_combined_trivia_video(main_question, voice, questions_json, background_video_path, logo_path, account_text, tictac_sound_path, ding_sound_path, output_file, question_font_path, options_font_path, account_font_path, question_image_font_path):
    start_time = time.time()  # Inicia el temporizador

    all_clips = []
    temp_files = []

    for idx, question in enumerate(questions_json):
        question_text = question['question_text']
        question_image = question['question_image']
        options = question['options']
        correct_option_index = question['correct_option_index']

        narration_text = f"쯭question_text}?"
        narration_text_winner = f"{options[correct_option_index]}!!"

        # Aqu칤 normalmente usar칤as generate_trivia_video para generar el clip con MoviePy
        # En lugar de eso, construir치s un comando ffmpeg para procesar cada clip
        temp_output = f"temp_clip_{idx}.mp4"

        ffmpeg_command = [
            'ffmpeg',
            '-y',  # Sobrescribir el archivo de salida si existe
            '-i', background_video_path,
            '-vf', f"drawtext=fontfile={question_font_path}:text='{question_text}':x=(w-text_w)/2:y=(h-text_h)/3",
            '-c:v', 'libx264',
            '-preset', 'faster', # 'ultrafast',
            '-c:a', 'aac',
            '-b:a', '192k',
            temp_output
        ]

        # Ejecutar el comando ffmpeg para generar cada clip
        subprocess.run(ffmpeg_command)
        temp_files.append(temp_output)

    # Crear el archivo de lista para ffmpeg
    with open("input.txt", "w") as f:
        for temp_file in temp_files:
            f.write(f"file '{os.path.abspath(temp_file)}'\n")

    # Concatenar los clips con ffmpeg
    ffmpeg_concat_command = [
        'ffmpeg',
        '-y',  # Sobrescribir el archivo de salida si existe
        '-f', 'concat',  # Usar el archivo de lista para concatenaci칩n
        '-safe', '0',
        '-i', 'input.txt',
        '-c:v', 'libx264',
        '-preset', 'faster', #'ultrafast',
        '-c:a', 'aac',
        '-b:a', '192k',
        output_file
    ]

    process = subprocess.Popen(ffmpeg_concat_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)

    # Mostrar barra de progreso
    for line in process.stderr:
        if "frame=" in line or "time=" in line:
            print(line.strip())  # Mostrar l칤neas de progreso de FFmpeg

    process.wait()  # Esperar a que termine el proceso FFmpeg

    # Limpiar archivos temporales
    for temp_file in temp_files:
        os.remove(temp_file)
    os.remove("input.txt")

    end_time = time.time()  # Detener el temporizador
    processing_time = end_time - start_time  # Calcular tiempo de procesamiento

    print(f"Tiempo de procesamiento: {processing_time} segundos")

# Funci칩n para generar un video con m칰ltiples preguntas
def generate_combined_trivia_video(main_question, voice, questions_json, background_video_path, logo_path, account_text, tictac_sound_path, ding_sound_path, output_file, question_font_path, options_font_path, account_font_path, question_image_font_path):
    start_time = time.time()  # Inicia el temporizador

    all_clips = []

    for question in questions_json:
        question_text = question['question_text']
        question_image = question['question_image']
        options = question['options']
        correct_option_index = question['correct_option_index']

        narration_text = f"쯭question_text}?"
        narration_text_winner = f"{options[correct_option_index]}!!"
        # narration_text_winner = f"Es, {options[correct_option_index]}!"

        trivia_clip = generate_trivia_video(
            main_question=main_question,
            voice=voice,
            background_video_path=background_video_path,
            logo_path=logo_path,
            question_text=question_text,
            question_image=question_image,
            options=options,
            correct_option_index=correct_option_index,
            account_text=account_text,
            narration_text=narration_text,
            narration_text_winner=narration_text_winner,
            tictac_sound_path=tictac_sound_path,
            ding_sound_path=ding_sound_path,
            question_font_path=question_font_path,
            options_font_path=options_font_path,
            account_font_path=account_font_path,
            question_image_font_path=question_image_font_path
        )

        all_clips.append(trivia_clip)

    for idx, clip in enumerate(all_clips):
        print(f"[DEBUG] all_clips[{idx}] - start: {clip.start}, duration: {clip.duration}, end: {clip.end}")

    # Combinar todos los clips en un solo video
    final_video = concatenate_videoclips(all_clips, method="compose")
    print(f"[DEBUG] generate_combined_trivia_video - final_video.duration: {final_video.duration}")

    # Guardar el video final
    # final_video = final_video.subclip(0, 4)
    final_video.write_videofile(output_file, codec="libx264", audio_codec="aac", fps=12, preset='ultrafast')

    end_time = time.time()  # Detiene el temporizador
    processing_time = end_time - start_time  # Calcula el tiempo de procesamiento

    print(f"Tiempo de procesamiento: {processing_time} segundos")


def generate_quiz_questions(theme, num_questions=2, num_options=4):
    # Genera la lista de opciones
    options_list = [f'"Opci칩n {i+1}"' for i in range(num_options)]

    # Determina si se debe incluir "emoji" en la pregunta
    emoji_included = 'emoji' in theme.lower()

    # Construye el prompt para la IA
    prompt = (
        f"Genera una lista de {num_questions} preguntas cortas, con cierto toque de sarcasmo por lo f치cil de responder, en formato JSON para un quiz / trivia sobre '{theme}'. "
        f"El JSON debe incluir una pregunta principal corta sin emojis en ella, usando un toque de sarcasmo y con el estilo MrBeast en la propiedad 'main_question' que aplique para todas las preguntas de la propiedad 'questions', "
        f"seguido de un array 'questions' que contenga objetos con las siguientes propiedades: "
        f"el texto de la pregunta con variantes {'mencionando siempre el tema central en cada pregunta ' if not emoji_included else ''} para hacer la pronunciaci칩n m치s humana y consistente {'SIN INCLUIR EL EMOJI' if emoji_included else ''}, "
        f"{'una imagen representada como un emoji,' if emoji_included else ''} "
        f"opciones de respuesta con {num_options} opciones donde alternes la opci칩n correcta en las diferentes posiciones del arreglo de opciones entre 0 y {num_options - 1}, (IMPORTANTE: deben tener un tamano de 18 caracteres m치ximo por opci칩n), y el 칤ndice de la opci칩n correcta (IMPORTANTE: Las opciones correctas deben est치r en posiciones aleatorias entre 0 y {num_options - 1} dentro del arreglo de la propiedad 'options').\n\n"
        f"Aqu칤 hay un ejemplo del formato:\n\n"
        "{\n"
        "  \"main_question\": \"Pregunta principal de ejemplo?\",\n"
        "  \"questions\": [\n"
        "    {\n"
        f"        \"question_text\": \"Variantes de la pregunta de ejemplo?\",\n"
        f"        \"question_image\": \"{'游뾇릖' if emoji_included else ''}\",\n"
        f"        \"options\": {options_list},\n"
        f"        \"correct_option_index\": 2\n"
        "    }\n"
        "  ]\n"
        "} \n\n"
        f"Notas Importantes: "
        "- En las opciones distribuye de forma equitativa y no secuencial la asignaci칩n de las opciones correctas en posiciones entre 0 y {num_options - 1}.\n"
        "- Todos los campos son obligatorios que est칠n presentes aunque est칠n en blanco.\n"
        "- REALIZA UNA DOBE VERIFICACION DE LAS OPCIONES CORRECTAS. NO PUEDES COMETER ERRORES.\n"
    )

    # print(prompt)

    messages = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": f"Eres una 칰til asistente virtual que ayuda a generar trivias y quizes educativas para videos en redes sociales.",
                },
            ],
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": prompt,
                },
            ],
        }
    ]

    print(prompt)

    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.7,
        response_format={"type": "json_object"}
    )

    result = response.choices[0].message.content

    return json.loads(result)


def main():
    # Configura el analizador de argumentos de l칤nea de comandos
    parser = argparse.ArgumentParser(description="Genera un video de trivia basado en las preguntas del quiz.")
    parser.add_argument("main_question", type=str, help="La pregunta principal del quiz.")
    parser.add_argument("num_questions", type=int, help="El n칰mero de preguntas en el quiz.")
    parser.add_argument("num_options", type=int, help="El n칰mero de opciones por pregunta.")

    args = parser.parse_args()

    # Genera el c칩digo UUID para el archivo de salida
    uuidcode = str(uuid.uuid4())

    # Genera las preguntas del quiz usando los argumentos proporcionados
    trivia = generate_quiz_questions(args.main_question, num_questions=args.num_questions, num_options=args.num_options)

    print(trivia)

    # Genera el video combinado de la trivia
    generate_combined_trivia_video(
        main_question=trivia["main_question"],
        voice="Pedro",
        questions_json=trivia["questions"],
        background_video_path="./assets/videos/background1.mp4",
        logo_path="./assets/images/logo.png",
        account_text="@elclubdelosgenios",
        tictac_sound_path="./assets/audios/clock.mp3",
        ding_sound_path="./assets/audios/ding.mp3",
        output_file=f"./videos/{uuidcode}.mp4",
        question_font_path="./assets/fonts/TT-Milks-Casual-Pie-Trial-Base.otf",
        options_font_path="./assets/fonts/Sniglet-Regular.ttf",
        account_font_path="./assets/fonts/Sniglet-Regular.ttf",
        question_image_font_path="./assets/fonts/AppleColorEmoji.ttf"
    )

    # Opcional: Imprime el JSON de la trivia
    # print(json.dumps(trivia, indent=4, ensure_ascii=False))

if __name__ == "__main__":
    main()

